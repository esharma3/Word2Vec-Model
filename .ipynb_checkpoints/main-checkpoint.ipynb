{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pre-trained Gensim Word2Vec model to find similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"\"\"I’m honored to be with you today because, let’s face it, you accomplished something I never could. If I get through this speech, it’ll be the first time I actually finish something at Harvard. Class of 2017, congratulations!\n",
    "\n",
    "I’m an unlikely speaker, not just because I dropped out, but because we’re technically in the same generation. We walked this yard less than a decade apart, studied the same ideas and slept through the same Ec10 lectures. We may have taken different paths to get here, especially if you came all the way from the Quad, but today I want to share what I’ve learned about our generation and the world we’re building together.\n",
    "\n",
    "But first, the last couple of days have brought back a lot of good memories.\n",
    "\n",
    "How many of you remember exactly what you were doing when you got that email telling you that you got into Harvard? I was playing Civilization and I ran downstairs, got my dad, and for some reason, his reaction was to video me opening the email. That could have been a really sad video. I swear getting into Harvard is still the thing my parents are most proud of me for.\n",
    "\n",
    "What about your first lecture at Harvard? Mine was Computer Science 121 with the incredible Harry Lewis. I was late so I threw on a t-shirt and didn’t realize until afterwards it was inside out and backwards with my tag sticking out the front. I couldn’t figure out why no one would talk to me — except one guy, KX Jin, he just went with it. We ended up doing our problem sets together, and now he runs a big part of Facebook. And that, Class of 2017, is why you should be nice to people.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "text = para.lower()\n",
    "text = re.sub(r\"\\d+\", \" \", text)\n",
    "text = re.sub(r\"\\s+\", \" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i’m honored to be with you today because, let’s face it, you accomplished something i never could.', 'if i get through this speech, it’ll be the first time i actually finish something at harvard.', 'class of , congratulations!', 'i’m an unlikely speaker, not just because i dropped out, but because we’re technically in the same generation.', 'we walked this yard less than a decade apart, studied the same ideas and slept through the same ec lectures.', 'we may have taken different paths to get here, especially if you came all the way from the quad, but today i want to share what i’ve learned about our generation and the world we’re building together.', 'but first, the last couple of days have brought back a lot of good memories.', 'how many of you remember exactly what you were doing when you got that email telling you that you got into harvard?', 'i was playing civilization and i ran downstairs, got my dad, and for some reason, his reaction was to video me opening the email.', 'that could have been a really sad video.', 'i swear getting into harvard is still the thing my parents are most proud of me for.', 'what about your first lecture at harvard?', 'mine was computer science with the incredible harry lewis.', 'i was late so i threw on a t-shirt and didn’t realize until afterwards it was inside out and backwards with my tag sticking out the front.', 'i couldn’t figure out why no one would talk to me — except one guy, kx jin, he just went with it.', 'we ended up doing our problem sets together, and now he runs a big part of facebook.', 'and that, class of , is why you should be nice to people.']\n"
     ]
    }
   ],
   "source": [
    "# creating a list of sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', '’', 'm', 'honored', 'to', 'be', 'with', 'you', 'today', 'because', ',', 'let', '’', 's', 'face', 'it', ',', 'you', 'accomplished', 'something', 'i', 'never', 'could', '.'], ['if', 'i', 'get', 'through', 'this', 'speech', ',', 'it', '’', 'll', 'be', 'the', 'first', 'time', 'i', 'actually', 'finish', 'something', 'at', 'harvard', '.'], ['class', 'of', ',', 'congratulations', '!'], ['i', '’', 'm', 'an', 'unlikely', 'speaker', ',', 'not', 'just', 'because', 'i', 'dropped', 'out', ',', 'but', 'because', 'we', '’', 're', 'technically', 'in', 'the', 'same', 'generation', '.'], ['we', 'walked', 'this', 'yard', 'less', 'than', 'a', 'decade', 'apart', ',', 'studied', 'the', 'same', 'ideas', 'and', 'slept', 'through', 'the', 'same', 'ec', 'lectures', '.'], ['we', 'may', 'have', 'taken', 'different', 'paths', 'to', 'get', 'here', ',', 'especially', 'if', 'you', 'came', 'all', 'the', 'way', 'from', 'the', 'quad', ',', 'but', 'today', 'i', 'want', 'to', 'share', 'what', 'i', '’', 've', 'learned', 'about', 'our', 'generation', 'and', 'the', 'world', 'we', '’', 're', 'building', 'together', '.'], ['but', 'first', ',', 'the', 'last', 'couple', 'of', 'days', 'have', 'brought', 'back', 'a', 'lot', 'of', 'good', 'memories', '.'], ['how', 'many', 'of', 'you', 'remember', 'exactly', 'what', 'you', 'were', 'doing', 'when', 'you', 'got', 'that', 'email', 'telling', 'you', 'that', 'you', 'got', 'into', 'harvard', '?'], ['i', 'was', 'playing', 'civilization', 'and', 'i', 'ran', 'downstairs', ',', 'got', 'my', 'dad', ',', 'and', 'for', 'some', 'reason', ',', 'his', 'reaction', 'was', 'to', 'video', 'me', 'opening', 'the', 'email', '.'], ['that', 'could', 'have', 'been', 'a', 'really', 'sad', 'video', '.'], ['i', 'swear', 'getting', 'into', 'harvard', 'is', 'still', 'the', 'thing', 'my', 'parents', 'are', 'most', 'proud', 'of', 'me', 'for', '.'], ['what', 'about', 'your', 'first', 'lecture', 'at', 'harvard', '?'], ['mine', 'was', 'computer', 'science', 'with', 'the', 'incredible', 'harry', 'lewis', '.'], ['i', 'was', 'late', 'so', 'i', 'threw', 'on', 'a', 't-shirt', 'and', 'didn', '’', 't', 'realize', 'until', 'afterwards', 'it', 'was', 'inside', 'out', 'and', 'backwards', 'with', 'my', 'tag', 'sticking', 'out', 'the', 'front', '.'], ['i', 'couldn', '’', 't', 'figure', 'out', 'why', 'no', 'one', 'would', 'talk', 'to', 'me', '—', 'except', 'one', 'guy', ',', 'kx', 'jin', ',', 'he', 'just', 'went', 'with', 'it', '.'], ['we', 'ended', 'up', 'doing', 'our', 'problem', 'sets', 'together', ',', 'and', 'now', 'he', 'runs', 'a', 'big', 'part', 'of', 'facebook', '.'], ['and', 'that', ',', 'class', 'of', ',', 'is', 'why', 'you', 'should', 'be', 'nice', 'to', 'people', '.']]\n"
     ]
    }
   ],
   "source": [
    "# further converting each sentence into a list of words\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['’', 'honored', 'today', ',', 'let', '’', 'face', ',', 'accomplished', 'something', 'never', 'could', '.'], ['get', 'speech', ',', '’', 'first', 'time', 'actually', 'finish', 'something', 'harvard', '.'], ['class', ',', 'congratulations', '!'], ['’', 'unlikely', 'speaker', ',', 'dropped', ',', '’', 'technically', 'generation', '.'], ['walked', 'yard', 'less', 'decade', 'apart', ',', 'studied', 'ideas', 'slept', 'ec', 'lectures', '.'], ['may', 'taken', 'different', 'paths', 'get', ',', 'especially', 'came', 'way', 'quad', ',', 'today', 'want', 'share', '’', 'learned', 'generation', 'world', '’', 'building', 'together', '.'], ['first', ',', 'last', 'couple', 'days', 'brought', 'back', 'lot', 'good', 'memories', '.'], ['many', 'remember', 'exactly', 'got', 'email', 'telling', 'got', 'harvard', '?'], ['playing', 'civilization', 'ran', 'downstairs', ',', 'got', 'dad', ',', 'reason', ',', 'reaction', 'video', 'opening', 'email', '.'], ['could', 'really', 'sad', 'video', '.'], ['swear', 'getting', 'harvard', 'still', 'thing', 'parents', 'proud', '.'], ['first', 'lecture', 'harvard', '?'], ['mine', 'computer', 'science', 'incredible', 'harry', 'lewis', '.'], ['late', 'threw', 't-shirt', '’', 'realize', 'afterwards', 'inside', 'backwards', 'tag', 'sticking', 'front', '.'], ['’', 'figure', 'one', 'would', 'talk', '—', 'except', 'one', 'guy', ',', 'kx', 'jin', ',', 'went', '.'], ['ended', 'problem', 'sets', 'together', ',', 'runs', 'big', 'part', 'facebook', '.'], [',', 'class', ',', 'nice', 'people', '.']]\n"
     ]
    }
   ],
   "source": [
    "# dropping stopwords from each list of words (every sentence is converted into a list of words) \n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word for word in sentences[i] if word not in stopwords.words(\"english\")]\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the word is not present at least 2 times, remove (ignore) it\n",
    "model = Word2Vec(sentences, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = model.wv.vocab\n",
    "# print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00303428  0.00404894 -0.00244913 -0.00248026  0.003996    0.00236965\n",
      "  0.00422836  0.00148972 -0.00062612  0.00204408  0.00439603 -0.00015394\n",
      "  0.00068051  0.00092499  0.00035283 -0.00216949 -0.00144381  0.00324738\n",
      "  0.00479151 -0.00085619  0.00079029  0.00318717  0.00440426  0.00076404\n",
      "  0.00453461 -0.00461847  0.00090625  0.00193784 -0.00093581 -0.0036896\n",
      "  0.00215838  0.00172147 -0.00269235  0.00214148 -0.00178107 -0.00107924\n",
      " -0.00264406  0.00189476  0.00256045  0.00371318 -0.00494314 -0.00342586\n",
      "  0.00387853 -0.00180101 -0.00496756 -0.00333493 -0.00088091  0.00376276\n",
      " -0.00103155 -0.00334114 -0.00294978 -0.0020412   0.00363728  0.00117224\n",
      " -0.00323676  0.00232899  0.00240414 -0.00290166  0.00282261  0.00272644\n",
      "  0.00230327  0.00086343  0.00496699 -0.00374963  0.00473863 -0.00480256\n",
      " -0.00139065 -0.0007083  -0.00330416  0.0012048  -0.00478155  0.00493229\n",
      " -0.00205918 -0.0043739  -0.00412051  0.00194477  0.00481531 -0.00483341\n",
      " -0.00376711  0.00272676  0.00465771 -0.00017847 -0.00131058 -0.00439644\n",
      "  0.00197838  0.00156806 -0.00492207 -0.00062077 -0.00311884  0.00333423\n",
      "  0.00105543 -0.00327452  0.00246616  0.00416829 -0.00362209 -0.00060366\n",
      " -0.00155329 -0.00262497 -0.00039774 -0.00148443]\n"
     ]
    }
   ],
   "source": [
    "# finding word vector\n",
    "vector = model.wv[\"harvard\"]\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('generation', 0.1920119673013687), (',', 0.1262374222278595), ('get', 0.06421398371458054), ('’', 0.051325954496860504), ('got', 0.046337973326444626), ('?', 0.0068762339651584625), ('could', -0.008338935673236847), ('today', -0.02444247342646122), ('together', -0.030627883970737457), ('class', -0.0313822478055954)]\n"
     ]
    }
   ],
   "source": [
    "# most similar words\n",
    "similar = model.wv.most_similar(\"harvard\")\n",
    "print(similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "word2vec",
   "language": "python",
   "name": "word2vec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
